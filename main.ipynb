{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n",
    "\n",
    "image = Image.open(IMAGE_PATH)\n",
    "langs = [\"en\"] # Replace with your languages or pass None (recommended to use None)\n",
    "recognition_predictor = RecognitionPredictor()\n",
    "detection_predictor = DetectionPredictor()\n",
    "\n",
    "predictions = recognition_predictor([image], [langs], detection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model_dict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m text_from_rendered\n\u001b[32m      5\u001b[39m converter = PdfConverter(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     artifact_dict=\u001b[43mcreate_model_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m rendered = converter(\u001b[33m\"\u001b[39m\u001b[33mFILEPATH\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m text, _, images = text_from_rendered(rendered)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/marker/models.py:14\u001b[39m, in \u001b[36mcreate_model_dict\u001b[39m\u001b[34m(device, dtype)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_model_dict\u001b[39m(device=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlayout_model\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mLayoutPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtexify_model\u001b[39m\u001b[33m\"\u001b[39m: TexifyPredictor(device=device, dtype=dtype),\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrecognition_model\u001b[39m\u001b[33m\"\u001b[39m: RecognitionPredictor(device=device, dtype=dtype),\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtable_rec_model\u001b[39m\u001b[33m\"\u001b[39m: TableRecPredictor(device=device, dtype=dtype),\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdetection_model\u001b[39m\u001b[33m\"\u001b[39m: DetectionPredictor(device=device, dtype=dtype),\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minline_detection_model\u001b[39m\u001b[33m\"\u001b[39m: InlineDetectionPredictor(device=device, dtype=dtype),\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mocr_error_model\u001b[39m\u001b[33m\"\u001b[39m: OCRErrorPredictor(device=device, dtype=dtype)\n\u001b[32m     21\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/.venv/lib/python3.12/site-packages/surya/common/predictor.py:24\u001b[39m, in \u001b[36mBasePredictor.__init__\u001b[39m\u001b[34m(self, checkpoint, device, dtype)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     22\u001b[39m loader = \u001b[38;5;28mself\u001b[39m.model_loader_cls(checkpoint)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = loader.processor()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/.venv/lib/python3.12/site-packages/surya/layout/loader.py:38\u001b[39m, in \u001b[36mLayoutModelLoader.model\u001b[39m\u001b[34m(self, device, dtype)\u001b[39m\n\u001b[32m     35\u001b[39m encoder = DonutSwinLayoutConfig(**encoder_config)\n\u001b[32m     36\u001b[39m config.encoder = encoder\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m model = \u001b[43mSuryaLayoutModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m model = model.to(device)\n\u001b[32m     40\u001b[39m model = model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/.venv/lib/python3.12/site-packages/surya/common/s3.py:132\u001b[39m, in \u001b[36mS3DownloaderMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    130\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e  \u001b[38;5;66;03m# Reraise exception after max retries\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4333\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4330\u001b[39m config.name_or_path = pretrained_model_name_or_path\n\u001b[32m   4332\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4333\u001b[39m model_init_context = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/marker/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3736\u001b[39m, in \u001b[36mPreTrainedModel.get_init_context\u001b[39m\u001b[34m(cls, is_quantized, _is_ds_init_called)\u001b[39m\n\u001b[32m   3734\u001b[39m         init_contexts.append(set_quantized_state())\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     init_contexts = [no_init_weights(), \u001b[43minit_empty_weights\u001b[49m()]\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[31mNameError\u001b[39m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter = PdfConverter(\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "rendered = converter(\"FILEPATH\")\n",
    "text, _, images = text_from_rendered(rendered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
